# w251-hw9: Distributed Training

In this homework, we are focused on aspects of multi-node and multi-gpu (mnmg) model training.
The high level idea is to practice running multi-node training by adapting the code we develop in homework 5 (imagenet training from random weights) to run on two GPU nodes instead of one.

Notes:
* Provisioned two g4dn.2xlarge instances. Each has 8 vCPUs, so you'll need to the ability (limit) to provision 16 vCPUs. 
* Added 400GB of storage space under /root

### Multi-Node
Files in `slave` and `master` folders

### Single-Node
Single.ipynb
